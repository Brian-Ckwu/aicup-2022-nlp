{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import load_json\n",
    "from arguments import PreprocessArgs\n",
    "from preprocess import Preprocessor\n",
    "\n",
    "data_path = Path(\"./dataset/train.csv\")\n",
    "data = pd.read_csv(data_path).drop([\"Unnamed: 6\", \"total no.: 7987\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,[\"q\", \"r\", \"s\", \"q'\", \"r'\"]] = data[[\"q\", \"r\", \"s\", \"q'\", \"r'\"]].applymap(lambda s: s.strip('\"'))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_labeled_span_indices(ref: str, ans: str) -> List[List[int]]:\n",
    "    s = 0\n",
    "    e = len(ans)\n",
    "    spans = list()\n",
    "    while (s < len(ans)):\n",
    "        cur_span = ans[s:e]\n",
    "        span_s = ref.find(cur_span)\n",
    "        if (span_s != -1):\n",
    "            spans.append([span_s, span_s + len(cur_span)])\n",
    "            s = e\n",
    "            e = len(ans)\n",
    "        else:\n",
    "            e = e - 1\n",
    "    return spans\n",
    "\n",
    "labeled_span_indices_l = {\n",
    "    \"q\": list(),\n",
    "    \"r\": list()\n",
    "}\n",
    "\n",
    "for field in ['q', 'r']:\n",
    "    for i, row in tqdm(data.iterrows()):\n",
    "        ref = row[field]\n",
    "        ans = row[field + \"'\"]\n",
    "        spans = get_labeled_span_indices(ref, ans)\n",
    "        labeled_span_indices_l[field].append(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_count = {\n",
    "    'q': 0,\n",
    "    'r': 0\n",
    "}\n",
    "\n",
    "for field in ['q', 'r']:\n",
    "    spans_l = labeled_span_indices_l[field]\n",
    "    for spans in spans_l:\n",
    "        if len(spans) == 1:\n",
    "            consecutive_count[field] += 1\n",
    "\n",
    "consecutive_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def label_sequence(ref: str, ans: str, ref_offsets: List[Tuple[int]]):\n",
    "    labels = [0] * len(ref_offsets)\n",
    "    labeled_spans = get_labeled_span_indices(ref, ans)\n",
    "\n",
    "    if not labeled_spans:\n",
    "        return labels\n",
    "\n",
    "    cur = 0\n",
    "    for i in range(len(ref_offsets)):\n",
    "        cur_labeled_span = labeled_spans[cur]\n",
    "        ref_offset = ref_offsets[i]\n",
    "        if (cur_labeled_span[0] <= ref_offset[0]) and (ref_offset[1] <= cur_labeled_span[1]): # if the ref_offset is in the current labeled span\n",
    "            labels[i] = 1\n",
    "        elif (ref_offset[0] < cur_labeled_span[0]): # if the ref_offset is to the left of current labeled span\n",
    "            pass # do nothing\n",
    "        elif (ref_offset[1] > cur_labeled_span[1]):\n",
    "            cur += 1\n",
    "            if cur >= len(labeled_spans):\n",
    "                break\n",
    "        else:\n",
    "            raise Exception(\"This condition should be happen.\")\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schemes = PreprocessArgs.input_schemes\n",
    "output_schemes = PreprocessArgs.output_schemes\n",
    "labeling_schemes = PreprocessArgs.labeling_schemes # 1: use the same class for q' and r' / 2: use different classes (e.g., I-q and I-r) for q' and r'\n",
    "\n",
    "args = PreprocessArgs(\n",
    "    use_nltk=False,\n",
    "    model_tokenizer_name=\"bert-base-uncased\",\n",
    "    input_scheme=\"qr\",\n",
    "    output_scheme=\"q'r'\",\n",
    "    labeling_scheme=\"IO1\"\n",
    ")\n",
    "\n",
    "preprocessor = Preprocessor(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data = preprocessor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = {\n",
    "    'q': list(),\n",
    "    'r': list()\n",
    "}\n",
    "labeled_span_indices_l = list()\n",
    "\n",
    "for field in ['q', 'r']:\n",
    "    for i, row in tqdm(data.iterrows()):\n",
    "        ref = row[field]\n",
    "        ans = row[field + \"'\"]\n",
    "        _, ref_offset = preprocessor.model_tokenize(ref)\n",
    "        labeled_span_indices = get_labeled_span_indices(ref, ans)\n",
    "        labeled_span_indices_l.append(labeled_span_indices)\n",
    "        labels = label_sequence(ref, ans, ref_offset)\n",
    "        ys[field].append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.id == 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys['q'][3:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, R, S, QP, RP = [data[field] for field in [\"q\", \"r\", \"s\", \"q'\", \"r'\"]]\n",
    "\n",
    "for input_scheme in PreprocessArgs.input_schemes:\n",
    "    for output_scheme in PreprocessArgs.output_schemes:\n",
    "        for labeling_scheme in PreprocessArgs.labeling_schemes:\n",
    "            args = PreprocessArgs(\n",
    "                use_nltk=False,\n",
    "                model_tokenizer_name=\"bert-base-uncased\",\n",
    "                input_scheme=input_scheme,\n",
    "                output_scheme=output_scheme,\n",
    "                labeling_scheme=labeling_scheme\n",
    "            )\n",
    "            print(f\"\\nScheme: {input_scheme} / {output_scheme} / {labeling_scheme}\\n\")\n",
    "            preprocessor.set_args(args)\n",
    "\n",
    "            index = 15\n",
    "\n",
    "            q = preprocessor.model_tokenize(Q[index])\n",
    "            r = preprocessor.model_tokenize(R[index])\n",
    "            s = S[index]\n",
    "            qp = preprocessor.label_sequence(q, preprocessor.model_tokenize(QP[index]))\n",
    "            rp = preprocessor.label_sequence(r, preprocessor.model_tokenize(RP[index]))\n",
    "\n",
    "            X, y = preprocessor.format_data(q, r, s, qp, rp)\n",
    "\n",
    "            if type(y) == tuple:\n",
    "                y_cls, y_seq = y\n",
    "                print(f\"y_cls: {y_cls}\")\n",
    "            else:\n",
    "                y_seq = y\n",
    "\n",
    "            print(f\"X: {X} -> {' '.join(preprocessor.model_tokenizer.convert_ids_to_tokens(X))}\")\n",
    "            print(f\"y: {y_seq} -> {' '.join(preprocessor.model_tokenizer.convert_ids_to_tokens(np.array(X)[np.array(y_seq) != 0]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ids = load_json(file=\"./dataset/splitIds__splitBy-id_stratifyBy-s_train-0.6_valid-0.2_test-0.2_seed-42.json\")\n",
    "\n",
    "# train_data, valid_data, test_data = [data[data.id.isin(split_ids[split])] for split in [\"train\", \"valid\", \"test\"]]\n",
    "# assert len(train_data) + len(valid_data) + len(test_data) == len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_all_answers = data[[\"id\", \"q'\", \"r'\"]]\n",
    "our_all_answers.to_csv(Path(\"./dataset/our_all_answers.csv\"), index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_df = pd.read_csv(Path(\"./dataset/our_all_answers.csv\"), names=[\"id\", \"q'\", \"r'\"])\n",
    "pred_df = ans_df.groupby(\"id\").sample()\n",
    "pred_df.to_csv(Path(\"./dataset/our_all_predictions_allqr.csv\"), index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Splits for Our Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def check_splits(splits: List[Dict[str, List[int]]], cv_ids: List[int]):\n",
    "    valid_ids_across_splits = list()\n",
    "    for split in splits:\n",
    "        assert sorted(split[\"train\"] + split[\"valid\"]) == cv_ids\n",
    "        valid_ids_across_splits += split[\"valid\"]\n",
    "\n",
    "    assert sorted(valid_ids_across_splits) == cv_ids\n",
    "\n",
    "# Make 1 cross validation split\n",
    "def make_cv_splits(cv_ids: List[int], cv_data: pd.DataFrame, n_splits: int, seed: int) -> List[Dict[str, List[int]]]:\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    print(f\"Currently making CV splits of seed {seed}: \\n{skf}\")\n",
    "\n",
    "    splits = list()\n",
    "    for train_index, valid_index in skf.split(X=cv_ids, y=cv_data.groupby(\"id\").first().s.values):\n",
    "        train_ids = sorted([cv_ids[index] for index in train_index])\n",
    "        valid_ids = sorted([cv_ids[index] for index in valid_index])\n",
    "        assert sorted(train_ids + valid_ids) == cv_ids\n",
    "\n",
    "        split = {\n",
    "            \"train\": train_ids,\n",
    "            \"valid\": valid_ids\n",
    "        }\n",
    "        splits.append(split)\n",
    "    \n",
    "    check_splits(splits, cv_ids)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently making CV splits of seed 0: \n",
      "StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
      "Currently making CV splits of seed 1: \n",
      "StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
      "Currently making CV splits of seed 2: \n",
      "StratifiedKFold(n_splits=5, random_state=2, shuffle=True)\n",
      "Currently making CV splits of seed 3: \n",
      "StratifiedKFold(n_splits=5, random_state=3, shuffle=True)\n",
      "Currently making CV splits of seed 4: \n",
      "StratifiedKFold(n_splits=5, random_state=4, shuffle=True)\n",
      "Currently making CV splits of seed 5: \n",
      "StratifiedKFold(n_splits=5, random_state=5, shuffle=True)\n",
      "Currently making CV splits of seed 6: \n",
      "StratifiedKFold(n_splits=5, random_state=6, shuffle=True)\n",
      "Currently making CV splits of seed 7: \n",
      "StratifiedKFold(n_splits=5, random_state=7, shuffle=True)\n",
      "Currently making CV splits of seed 8: \n",
      "StratifiedKFold(n_splits=5, random_state=8, shuffle=True)\n",
      "Currently making CV splits of seed 9: \n",
      "StratifiedKFold(n_splits=5, random_state=9, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "# Merge train & valid splits for cross-validation\n",
    "cv_ids = sorted(split_ids[\"train\"] + split_ids[\"valid\"])\n",
    "assert len(set(cv_ids) & set(split_ids[\"test\"])) == 0\n",
    "cv_data = data[data.id.isin(cv_ids)]\n",
    "\n",
    "# Make 10 different cross validation splits\n",
    "n_splits = 5\n",
    "filepath_prefix = f\"./dataset/cross_validation/our_testing/splitIds__nsplits-{n_splits}\"\n",
    "seeds = range(10)\n",
    "for seed in seeds:\n",
    "    splits = make_cv_splits(cv_ids, cv_data, n_splits, seed)\n",
    "\n",
    "    filepath = f\"{filepath_prefix}_seed-{seed}.json\"\n",
    "    Path(filepath).write_text(data=json.dumps(obj=splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Splits for Final Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train, valid, and test splits for final cross-validation\n",
    "\n",
    "# Make 10 different cross validation splits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cuda-11.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35b66dd0c8f752918e1728d86abaa8fb004a7dee1d90779ea4d0023d852f9fe7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
