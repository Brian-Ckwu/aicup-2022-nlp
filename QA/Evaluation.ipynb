{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2021751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict\n",
    "from typing import List, Tuple, Any, Union\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from transformers import EvalPrediction\n",
    "#from preprocess import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3fc36e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenize(text: str, filter_puncts: bool = True) -> List[str]:\n",
    "    punctuations = set([ch for ch in \"!\\\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\"])\n",
    "    text = text.strip('\"') # NOTE: remove the quotes first\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    if filter_puncts:\n",
    "        tokens = list(filter(lambda t: t not in punctuations, tokens))\n",
    "    return tokens\n",
    "    \n",
    "def longestCommonSubsequence(text1: list, text2: list) -> int:\n",
    "    if len(text2) > len(text1):\n",
    "        text1, text2 = text2, text1\n",
    "\n",
    "    lcs = [[0] * (len(text2) + 1) for _ in range(2)]\n",
    "    for i in range(1, len(text1)+1):\n",
    "        for j in range(1, len(text2)+1):\n",
    "            if text1[i-1] == text2[j-1]:\n",
    "                lcs[i % 2][j] = lcs[(i-1) % 2][j-1] + 1\n",
    "            else:\n",
    "                lcs[i % 2][j] = max(lcs[(i-1) % 2][j], lcs[i % 2][j-1])\n",
    "\n",
    "    return lcs[len(text1) % 2][len(text2)]\n",
    "\n",
    "def compute_lcs_score(pred: list, ans: list) -> float:\n",
    "    intersection = longestCommonSubsequence(pred, ans)\n",
    "    union = len(pred) + len(ans) - intersection\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    lcs_score = intersection / union\n",
    "    if (lcs_score < 0) or (lcs_score) > 1:\n",
    "        raise ValueError(\"LCS score must be between 0 and 1\")\n",
    "    return lcs_score\n",
    "\n",
    "def compute_lcs_scores(pred_df: pd.DataFrame, ans_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ids, qp_scores, rp_scores = list(), list(), list()\n",
    "    for _, prow in pred_df.iterrows():\n",
    "        pid, qp_pred, rp_pred = prow[\"id\"], prow[\"q'\"], prow[\"r'\"]\n",
    "        qp_pred, rp_pred = [nltk_tokenize(pred) for pred in [qp_pred, rp_pred]]\n",
    "        ans_rows = ans_df[ans_df.id == pid]\n",
    "\n",
    "        for _, arow in ans_rows.iterrows():\n",
    "            qp_ans, rp_ans = arow[\"q'\"], arow[\"r'\"]\n",
    "            qp_ans, rp_ans = [nltk_tokenize(ans) for ans in [qp_ans, rp_ans]]\n",
    "            qp_score, rp_score = compute_lcs_score(qp_pred, qp_ans), compute_lcs_score(rp_pred, rp_ans)\n",
    "\n",
    "            for item, l in zip([pid, qp_score, rp_score], [ids, qp_scores, rp_scores]):\n",
    "                l.append(item)\n",
    "\n",
    "    assert ids == ans_df.id.tolist()\n",
    "    lcs_df = pd.DataFrame(data={\n",
    "        \"id\": ids,\n",
    "        \"qp_scores\": qp_scores,\n",
    "        \"rp_scores\": rp_scores\n",
    "    })\n",
    "    return lcs_df\n",
    "\n",
    "def compute_final_score(lcs_df: pd.DataFrame) -> float:\n",
    "    lcs_df[\"total_scores\"] = lcs_df[\"qp_scores\"] + lcs_df[\"rp_scores\"]\n",
    "    max_scores = lcs_df.groupby(\"id\")[\"total_scores\"].max()\n",
    "    final_score = max_scores.sum() / (2 * len(max_scores))\n",
    "    if (final_score < 0) or (final_score > 1):\n",
    "        raise ValueError(\"The final score must be between 0 and 1, please check the implementation.\")\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81c4bf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So taking peoples rights away is an american value ? I think I will push for legislation to make cancer surgery illegal .\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "pred_df = pd.read_csv(\"./outputs/predict_test.csv\", names=[\"id\", \"q'\", \"r'\"], dtype=str)\n",
    "ans_df = pd.read_csv(\"test_gold_622.csv\", names=[\"id\", \"q'\", \"r'\"], dtype=str)\n",
    "\n",
    "for i in range(pred_df.shape[0]):\n",
    "    tempq = str(pred_df.iloc[i][\"q'\"]).rsplit(' ', 1)[0]\n",
    "    tempr = str(pred_df.iloc[i][\"r'\"]).rsplit(' ', 1)[0]\n",
    "    pred_df.iloc[i][\"q'\"] = tempq\n",
    "    pred_df.iloc[i][\"r'\"] = tempr\n",
    "print(pred_df.iloc[5][\"q'\"])\n",
    "print(pred_df.isnull().values.any())\n",
    "print(ans_df.isnull().values.any())\n",
    "pred_df = pred_df.fillna('')\n",
    "print(pred_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "98eed5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test score: 0.754260262235276\n"
     ]
    }
   ],
   "source": [
    "if len(pred_df) != len(ans_df.groupby(\"id\").size()):\n",
    "    raise ValueError(\"The prediction file must have the same number of rows as the number of unique IDs in the answer file\")\n",
    "\n",
    "lcs_df = compute_lcs_scores(pred_df, ans_df) # has len(ans_df) rows of lcs_q' and lcs_r'\n",
    "final_score = compute_final_score(lcs_df) # derive the final score by \"1/2N (\\sum_i^N(max_j(score_q' + score_r')))\"\n",
    "print(f\"Final test score: {final_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f17071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### bert-large-uncased #####\n",
    "#500: 0.5845899012911759\n",
    "#1000: 0.5673322547362882\n",
    "#1500: 0.618273462886458\n",
    "#2000: 0.5929113627103975\n",
    "#2500: 0.5836752600091908\n",
    "#3000: 0.5987110975060392\n",
    "#3500: 0.5981470868846762\n",
    "#4000: 0.5981470868846762\n",
    "#4500: 0.5907715976593922\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e325d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypermutation would most likely just end up killing itself\n"
     ]
    }
   ],
   "source": [
    "a = 'hypermutation would most likely just end up killing itself .'\n",
    "print(a.rsplit(' ', 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e28cde05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once again , you seem to support the killing of certain people .\n",
      "alright how did evolution start ?\n",
      "Do you not even get statutory sick pay ?\n",
      "Why do you expect me to provide evidence when you never do ?\n",
      "What has caused you to believe such things ?\n",
      "So taking peoples rights away is an american value ? I think I will push for legislation to make cancer surgery illegal . After\n"
     ]
    }
   ],
   "source": [
    "pred_df = pd.read_csv(\"./outputs/predict_test.csv\", names=[\"id\", \"q'\", \"r'\"], dtype=str)\n",
    "\n",
    "for i in range(6):\n",
    "    print(str(pred_df.iloc[i][\"q'\"]))\n",
    "    tempq = str(pred_df.iloc[i][\"q'\"]).rsplit(' ', 1)[0]\n",
    "    tempr = str(pred_df.iloc[i][\"r'\"]).rsplit(' ', 1)[0]\n",
    "    pred_df.iloc[i][\"q'\"] = tempq\n",
    "    pred_df.iloc[i][\"r'\"] = tempr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c3ec8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So taking peoples rights away is an american value ? I think I will push for legislation to make cancer surgery illegal .\n"
     ]
    }
   ],
   "source": [
    "print(pred_df.iloc[5][\"q'\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0804d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
