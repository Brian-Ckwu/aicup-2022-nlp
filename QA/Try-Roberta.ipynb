{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6a2320a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22919 7728 7699\n",
      "30647 7699\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from re import search\n",
    "from typing import List, Tuple, Any, Union\n",
    "import nltk\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_json(file: str):\n",
    "    return json.loads(Path(file).read_bytes())\n",
    "\n",
    "p_data = pd.read_csv(\"./dataset/train.csv\").drop([\"Unnamed: 6\", \"total no.: 7987\"], axis=1)\n",
    "\n",
    "split_ids = load_json('./dataset/splitIds__splitBy-id_stratifyBy-s_train-0.6_valid-0.2_test-0.2_seed-42.json')\n",
    "# train_data, valid_data, test_data = [p_data[p_data.id.isin(split_ids[split])] for split in [\"train\", \"valid\", \"test\"]]\n",
    "# print(train_data.shape[0],valid_data.shape[0],test_data.shape[0])\n",
    "\n",
    "train_data, train2_data, valid_data = [p_data[p_data.id.isin(split_ids[split])] for split in [\"train\", \"valid\", \"test\"]]\n",
    "print(train_data.shape[0],train2_data.shape[0],valid_data.shape[0])\n",
    "train_data = pd.concat([train_data, train2_data],axis=0)\n",
    "print(train_data.shape[0], valid_data.shape[0])\n",
    "\n",
    "# split_ids = load_json('./dataset/splitIds__nsplits-5_seed-3.json')\n",
    "# train_data, valid_data = [p_data[p_data.id.isin(split_ids[1][split])] for split in [\"train\", \"valid\"]]\n",
    "# print(p_data.shape[0], train_data.shape[0],valid_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52386a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"max_len\" : 512,\n",
    "    \"batch_size\" : 8,\n",
    "    \"model_name\" : \"janeel/muppet-roberta-base-finetuned-squad\",\n",
    "    \"learning_rate\" : 3e-5,\n",
    "    \"warmup_ratio\" : 0.06,\n",
    "    \"seed\" : 24,\n",
    "    \"split\" : \"6+22\",\n",
    "    \"special\" : \"shuffle\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c648c",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd1d09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains(small, big):\n",
    "    for i in range(len(big)-len(small)+1):\n",
    "        for j in range(len(small)):\n",
    "            if big[i+j] != small[j]:\n",
    "                break\n",
    "        else:\n",
    "            return i, i+len(small)-1\n",
    "    return False\n",
    "\n",
    "def keep_continuous(data: pd.DataFrame):\n",
    "    keep=[]\n",
    "    for i in range(data.shape[0]):\n",
    "        qp_not_in_q = data.iloc[i]['q\\''][1:-1] not in data.iloc[i]['q'][1:-1]\n",
    "        rp_not_in_r = data.iloc[i]['r\\''][1:-1] not in data.iloc[i]['r'][1:-1]\n",
    "        if not (qp_not_in_q or rp_not_in_r):\n",
    "            keep.append(i)\n",
    "    \n",
    "    data = data.iloc[keep]\n",
    "    return data\n",
    "\n",
    "#format data \n",
    "#token type id sepid is in 0 not 1, context 0 question 1\n",
    "#attention mask有東西的1其他0\n",
    "#TODO: pad use tokenizer.pad_token_id\n",
    "def format_data_qp(q: List[int], r: List[int], s: int, qp: List[int], rp: List[int]) -> Tuple[List[int], List[int], List[int], int, int]:\n",
    "    cls_q_sep = [clsid] + q + [sepid]\n",
    "    \n",
    "    q_r_s = [clsid] + q + [sepid] + r + [sepid] + s + [sepid]\n",
    "    attention_mask = [1 if _ in range(len(q_r_s)) else 0 for _ in range(args['max_len'])]\n",
    "    input_id = [q_r_s[_] if _ in range(len(q_r_s)) else padid for _ in range(args['max_len'])]\n",
    "    \n",
    "    if contains(qp, q_r_s):\n",
    "        start_pos, end_pos = contains(qp, q_r_s)\n",
    "        print('qp:', qp)\n",
    "        print('q_r_s:', q_r_s)\n",
    "        print(start_pos, end_pos)\n",
    "    else:\n",
    "        start_pos, end_pos = 0, 0\n",
    "        \n",
    "    return input_id, attention_mask, start_pos, end_pos\n",
    "\n",
    "def format_data_rp(q: List[int], r: List[int], s: int, qp: List[int], rp: List[int]) -> Tuple[List[int], List[int], List[int], int, int]:\n",
    "    cls_q_sep = [clsid] + r + [sepid]\n",
    "    \n",
    "    q_r_s = [clsid] + r + [sepid] + q + [sepid] + s + [sepid]\n",
    "    attention_mask = [1 if _ in range(len(q_r_s)) else 0 for _ in range(args['max_len'])]\n",
    "    input_id = [q_r_s[_] if _ in range(len(q_r_s)) else padid for _ in range(args['max_len'])]\n",
    "    \n",
    "    if contains(rp, q_r_s):\n",
    "        start_pos, end_pos = contains(rp, q_r_s)\n",
    "    else:\n",
    "        start_pos, end_pos = 0, 0\n",
    "        \n",
    "    return input_id, attention_mask, start_pos, end_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22a822",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1429813",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args[\"model_name\"])\n",
    "clsid = tokenizer.cls_token_id\n",
    "sepid = tokenizer.sep_token_id\n",
    "padid = tokenizer.pad_token_id\n",
    "def model_tokenize(text: str) -> List[int]:\n",
    "    text = text.strip('\"')\n",
    "    token_ids = tokenizer(text)[\"input_ids\"]\n",
    "    return token_ids[1:-1] #without cls sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0e717f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 713, 16, 16, 10, 1040, 4, 2, 2, 713, 16, 16, 10, 1040, 80, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('This is is a book.', 'This is is a book two.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1fe7253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data: pd.DataFrame, choice:str):\n",
    "    data = keep_continuous(data)\n",
    "    print('ids left:', data['id'].nunique())\n",
    "    print('instances left', data.shape[0])\n",
    "    ids = list(data.id)\n",
    "    Q, R, S, QP, RP = [data[field] for field in [\"q\", \"r\", \"s\", \"q'\", \"r'\"]]\n",
    "    Q, R, QP, RP, S = [list(map(model_tokenize, x)) for x in [Q, R, QP, RP, S]]\n",
    "\n",
    "    # only keep those Q+R+S < 512 tokens\n",
    "    count = 0\n",
    "    keep = []\n",
    "    for i in range(len(Q)):\n",
    "        if (len(Q[i])+len(R[i])) > 512-5:\n",
    "            count += 1\n",
    "        else:\n",
    "            keep.append(i)\n",
    "    print(f\"Q+R+S longer than {args['max_len']} tokens:\", count, \" Remains:\",len(keep))\n",
    "    Q = [Q[i] for i in keep]\n",
    "    R = [R[i] for i in keep]\n",
    "    QP = [QP[i] for i in keep]\n",
    "    RP = [RP[i] for i in keep]\n",
    "    S = [S[i] for i in keep]\n",
    "    ids = [ids[i] for i in keep]\n",
    "    \n",
    "    #find start end positions make dict\n",
    "    if choice == 'qp':\n",
    "        data = list(map(format_data_qp, Q, R, S, QP, RP))\n",
    "    elif choice == 'rp':\n",
    "        data = list(map(format_data_rp, Q, R, S, QP, RP))\n",
    "    else:\n",
    "        return 'ERROR'\n",
    "    return 0\n",
    "    input_list, token_list, attention_list, s_pos, e_pos =[], [], [], [], []\n",
    "    for i in range(len(data)):\n",
    "        input_list.append(data[i][0])\n",
    "        attention_list.append(data[i][1])\n",
    "        s_pos.append(data[i][2])\n",
    "        e_pos.append(data[i][3])\n",
    "        \n",
    "    data = {\n",
    "        'input_ids': input_list,\n",
    "        'attention_masks': attention_list,\n",
    "        'start_positions': s_pos,\n",
    "        'end_positions': e_pos\n",
    "    }\n",
    "    \n",
    "    #make dataset\n",
    "    ds = Dataset.from_dict(data)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9499ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids left: 1\n",
      "instances left 3\n",
      "Q+R+S longer than 512 tokens: 0  Remains: 3\n",
      "qp: [243, 64, 213, 258, 1319, 479, 166, 70, 2980, 479, 85, 16, 99, 47, 109, 19, 24, 14, 3510, 479]\n",
      "q_r_s: [0, 243, 64, 213, 258, 1319, 479, 166, 70, 2980, 479, 85, 16, 99, 47, 109, 19, 24, 14, 3510, 479, 2, 36948, 479, 2, 3450, 30009, 2]\n",
      "1 20\n",
      "qp: [243, 64, 213, 258, 1319, 479, 166, 70, 2980, 479, 85, 16, 99, 47, 109, 19, 24, 14, 3510, 479]\n",
      "q_r_s: [0, 243, 64, 213, 258, 1319, 479, 166, 70, 2980, 479, 85, 16, 99, 47, 109, 19, 24, 14, 3510, 479, 2, 36948, 479, 2, 3450, 30009, 2]\n",
      "1 20\n"
     ]
    }
   ],
   "source": [
    "train_data_qp_done=preprocess(train_data, 'qp')\n",
    "valid_data_qp_done=preprocess(valid_data, 'qp')\n",
    "\n",
    "train_data_rp_done=preprocess(train_data, 'rp')\n",
    "valid_data_rp_done=preprocess(valid_data, 'rp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541aeecd",
   "metadata": {},
   "source": [
    "### Model / Collator / Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7980f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import default_data_collator\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(args[\"model_name\"])\n",
    "model_args = TrainingArguments(\n",
    "    f'{args[\"model_name\"]}-qp-82-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=args[\"learning_rate\"],\n",
    "    per_device_train_batch_size=args['batch_size'],\n",
    "    per_device_eval_batch_size=args['batch_size'],\n",
    "    warmup_ratio=args[\"warmup_ratio\"],\n",
    "    seed=args[\"seed\"],\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    model_args,\n",
    "    train_dataset=train_data_qp_done,\n",
    "    eval_dataset=valid_data_qp_done,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.save_model(f'aicup-trained-qp-{args[\"model_name\"]}-82')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efea988",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(args[\"model_name\"])\n",
    "model_args = TrainingArguments(\n",
    "    f'{args[\"model_name\"]}-rp-82-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=args[\"learning_rate\"],\n",
    "    per_device_train_batch_size=args['batch_size'],\n",
    "    per_device_eval_batch_size=args['batch_size'],\n",
    "    warmup_ratio=args[\"warmup_ratio\"],\n",
    "    seed=args[\"seed\"],\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    model_args,\n",
    "    train_dataset=train_data_rp_done,\n",
    "    eval_dataset=valid_data_rp_done,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.save_model(f'aicup-trained-rp-{args[\"model_name\"]}-82')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0089f4",
   "metadata": {},
   "source": [
    "### Predict Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c42c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_unique_id(data: pd.DataFrame):\n",
    "    ids = []\n",
    "    index = []\n",
    "    for i in range(data.shape[0]):\n",
    "        if data.iloc[i]['id'] not in ids:\n",
    "            ids.append(data.iloc[i]['id'])\n",
    "            index.append(i)\n",
    "    print(len(index), len(ids))\n",
    "    data = data.iloc[index]\n",
    "    return data\n",
    "\n",
    "def format_data_post_qp(q: str, r: str, s: str, ids: str):\n",
    "    q_r_s =  q + '</s>' + r + '</s>' + s \n",
    "    tokenized_q_r_s = tokenizer(q_r_s, return_offsets_mapping=True, padding=\"max_length\", max_length=512, truncation=True)\n",
    "    \n",
    "    cls_q_sep =  q\n",
    "    tokenized_q = tokenizer(cls_q_sep)[\"input_ids\"]\n",
    "    \n",
    "    tokenized_q_r_s[\"example_id\"] = ids\n",
    "    tokenized_q_r_s[\"offset_mapping\"] = [tokenized_q_r_s[\"offset_mapping\"][_] if _ in range(len(tokenized_q)-1) else None for _ in range(len(tokenized_q_r_s[\"offset_mapping\"]))]\n",
    "    tokenized_q_r_s[\"offset_mapping\"][0] = None\n",
    "    return tokenized_q_r_s\n",
    "\n",
    "def format_data_post_rp(q: str, r: str, s: str, ids: str):\n",
    "    q_r_s =  r + '</s>' + q + '</s>' + s \n",
    "    tokenized_q_r_s = tokenizer(q_r_s, return_offsets_mapping=True, padding=\"max_length\", max_length=512, truncation=True)\n",
    "    \n",
    "    cls_q_sep =  r\n",
    "    tokenized_q = tokenizer(cls_q_sep)[\"input_ids\"]\n",
    "    \n",
    "    tokenized_q_r_s[\"example_id\"] = ids\n",
    "    tokenized_q_r_s[\"offset_mapping\"] = [tokenized_q_r_s[\"offset_mapping\"][_] if _ in range(len(tokenized_q)-1) else None for _ in range(len(tokenized_q_r_s[\"offset_mapping\"]))]\n",
    "    tokenized_q_r_s[\"offset_mapping\"][0] = None\n",
    "    return tokenized_q_r_s\n",
    "        \n",
    "def postprocess(data: pd.DataFrame, choice: str):\n",
    "    #data = keep_continuous(data) #seems unusefull in predicting\n",
    "    print('ids left:', data['id'].nunique())\n",
    "    print('instances left', data.shape[0])\n",
    "    ids = list(data.id)\n",
    "    Q, R, S = [data[field] for field in [\"q\", \"r\", \"s\"]]\n",
    "    Q, R, S = [list(map(lambda x: x.strip('\"'), y)) for y in [Q, R, S]]\n",
    "    \n",
    "    if choice == 'qp':\n",
    "        data = list(map(format_data_post_qp, Q, R, S, ids))\n",
    "    elif choice == 'rp':\n",
    "        data = list(map(format_data_post_rp, Q, R, S, ids))\n",
    "    input_list, token_list, attention_list, offset, ex_id =[], [], [], [], []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        input_list.append(data[i][\"input_ids\"])\n",
    "        attention_list.append(data[i][\"attention_mask\"])\n",
    "        offset.append(data[i][\"offset_mapping\"])\n",
    "        ex_id.append(data[i][\"example_id\"])\n",
    "        \n",
    "    data = {\n",
    "        'input_ids': input_list,\n",
    "        'attention_mask': attention_list,\n",
    "        'offset_mapping': offset,\n",
    "        'example_id': ex_id\n",
    "    }\n",
    "    \n",
    "    #make dataset\n",
    "    ds = Dataset.from_dict(data)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# load model\n",
    "def getPredictFromCkpt(ckpt: str, choice: str, test_post):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(f'corrected_models/{args[\"model_name\"]}-{choice}-{args[\"split\"]}-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}-warm_{args[\"warmup_ratio\"]}-seed_{args[\"seed\"]}-{args[\"special\"]}/checkpoint-{ckpt}')\n",
    "    test_args = TrainingArguments(\n",
    "        output_dir = f'corrected_models/{args[\"model_name\"]}-{choice}-{args[\"split\"]}-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}-warm_{args[\"warmup_ratio\"]}-seed_{args[\"seed\"]}-{args[\"special\"]}/checkpoint-{ckpt}',\n",
    "        do_train = False,\n",
    "        do_predict = True,\n",
    "        per_device_eval_batch_size = args[\"batch_size\"],\n",
    "        gradient_accumulation_steps=2\n",
    "    )\n",
    "\n",
    "    # init trainer\n",
    "    trainer = Trainer(model = model, args = test_args)\n",
    "    raw_predictions = trainer.predict(test_post)\n",
    "    return raw_predictions\n",
    "\n",
    "# turn raw predictions (start/end span) to strings\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, choice, n_best_size = 10, max_answer_length = 510):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    n_best_size = 10\n",
    "    predictions = collections.OrderedDict()\n",
    "    #print(examples.shape[0], len(features))\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index in range(examples.shape[0]):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        valid_answers = []\n",
    "        \n",
    "        if choice == 'qp':\n",
    "            context = examples.iloc[example_index][\"q\"][1:-1] #strip \"\n",
    "        elif choice == 'rp':\n",
    "            context = examples.iloc[example_index][\"r\"][1:-1] #strip \"\n",
    "        #print(\"Q :\", context)\n",
    "        \n",
    "        # We grab the predictions of the model for this feature.\n",
    "        start_logits = all_start_logits[example_index]\n",
    "        end_logits = all_end_logits[example_index]\n",
    "        # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "        # context.\n",
    "        offset_mapping = features[example_index][\"offset_mapping\"]\n",
    "        #print(offset_mapping)\n",
    "        # Update minimum null prediction.\n",
    "        cls_index = features[example_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "        feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "\n",
    "        # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "        start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "        end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                # to part of the input_ids that are not in the context.\n",
    "                if (\n",
    "                    start_index >= len(offset_mapping)\n",
    "                    or end_index >= len(offset_mapping)\n",
    "                    or offset_mapping[start_index] is None\n",
    "                    or offset_mapping[end_index] is None\n",
    "                ):\n",
    "                    continue\n",
    "                # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                    continue\n",
    "\n",
    "                start_char = offset_mapping[start_index][0]\n",
    "                end_char = offset_mapping[end_index][1]\n",
    "                #print(start_index, end_index, start_char, end_char)\n",
    "                valid_answers.append(\n",
    "                    {\n",
    "                        \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                        \"text\": context[start_char: end_char] # +1 because of the starting \"\n",
    "                    }\n",
    "                )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "            # failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        #print(\"Q':\", best_answer[\"text\"])\n",
    "        #print(\"=================\")\n",
    "        predictions[examples.iloc[example_index][\"id\"]] = best_answer[\"text\"]\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "# make gold csv from valid/ test set data\n",
    "def make_gold_csv(data, output_name):\n",
    "    test_dropped = data.drop(['q', 'r', 's'], axis=1)\n",
    "    test_dropped = test_dropped.fillna('')\n",
    "    test_dropped.to_csv(output_name, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e85ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_gold_csv(valid_data,\"split82_3_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52722f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 2016\n",
      "ids left: 2016\n",
      "instances left 2016\n",
      "ids left: 2016\n",
      "instances left 2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file corrected_models/janeel/muppet-roberta-base-finetuned-squad-qp-6+22-b_8-lr_3e-05-warm_0.06-seed_24-shuffle/checkpoint-1600/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"corrected_models/janeel/muppet-roberta-base-finetuned-squad-qp-6+22-b_8-lr_3e-05-warm_0.06-seed_24-shuffle/checkpoint-1600\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file corrected_models/janeel/muppet-roberta-base-finetuned-squad-qp-6+22-b_8-lr_3e-05-warm_0.06-seed_24-shuffle/checkpoint-1600/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at corrected_models/janeel/muppet-roberta-base-finetuned-squad-qp-6+22-b_8-lr_3e-05-warm_0.06-seed_24-shuffle/checkpoint-1600.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2016\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file corrected_models/janeel/muppet-roberta-base-finetuned-squad-rp-6+22-b_8-lr_3e-05-warm_0.06-seed_24-shuffle/checkpoint-1600/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"corrected_models/janeel/muppet-roberta-base-finetuned-squad-rp-6+22-b_8-lr_3e-05-warm_0.06-seed_24-shuffle/checkpoint-1600\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file corrected_models/janeel/muppet-roberta-base-finetuned-squad-rp-6+22-b_8-lr_3e-05-warm_0.06-seed_24-shuffle/checkpoint-1600/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at corrected_models/janeel/muppet-roberta-base-finetuned-squad-rp-6+22-b_8-lr_3e-05-warm_0.06-seed_24-shuffle/checkpoint-1600.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2016\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions to csv done.\n"
     ]
    }
   ],
   "source": [
    "# 1 if we are going to predict the final submission test\n",
    "real_predict_test = 1\n",
    "if real_predict_test == 1:\n",
    "    valid_data = pd.read_csv(\"./dataset/test.csv\")\n",
    "\n",
    "test_data_unique = leave_unique_id(valid_data)\n",
    "test_qp_post = postprocess(test_data_unique, 'qp')\n",
    "test_rp_post = postprocess(test_data_unique, 'rp')\n",
    "\n",
    "ckpts = ['500', '1000', '1500', '2000', '2500', '3000', '3500']\n",
    "ckpts = list(range(400, 2800, 200))\n",
    "if real_predict_test == 1:\n",
    "    ckpts = ['1600']\n",
    "for ckpt in ckpts:\n",
    "    raw_predictions_qp = getPredictFromCkpt(ckpt, 'qp', test_qp_post)\n",
    "    raw_predictions_rp = getPredictFromCkpt(ckpt, 'rp', test_rp_post)\n",
    "    \n",
    "    test_qp_post.set_format(type=test_qp_post.format[\"type\"], columns=list(test_qp_post.features.keys()))\n",
    "    test_rp_post.set_format(type=test_rp_post.format[\"type\"], columns=list(test_rp_post.features.keys()))\n",
    "    \n",
    "    final_predictions_qp = postprocess_qa_predictions(test_data_unique, test_qp_post, raw_predictions_qp.predictions, 'qp')\n",
    "    final_predictions_rp = postprocess_qa_predictions(test_data_unique, test_rp_post, raw_predictions_rp.predictions, 'rp')\n",
    "    #print(final_predictions_qp)\n",
    "    ids, qp, rp = [], [], []\n",
    "    for k,v in final_predictions_qp.items():\n",
    "        ids.append(k)\n",
    "        qp.append(v)\n",
    "    for k,v in final_predictions_rp.items():\n",
    "        rp.append(v)\n",
    "\n",
    "    dict = {'id': ids, \"q'\": qp, \"r'\": rp} \n",
    "    df = pd.DataFrame(dict) \n",
    "    df = df.replace(np.nan, '', regex=True)\n",
    "    if real_predict_test == 1:\n",
    "        df.to_csv(f'./outputs/corrected_models/HALF_predict_test_muppet-roberta-base-finetuned-squad-{args[\"split\"]}-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}-warm_{args[\"warmup_ratio\"]}-seed_{args[\"seed\"]}-{args[\"special\"]}-checkpoint-{ckpt}.csv',header=False)\n",
    "    else:\n",
    "        df.to_csv(f'./outputs/corrected_models/predict_test_{args[\"model_name\"]}-{args[\"split\"]}-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}-warm_{args[\"warmup_ratio\"]}-seed_{args[\"seed\"]}-{args[\"special\"]}-checkpoint-{ckpt}.csv',header=False)\n",
    "    print(\"Predictions to csv done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0e46f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any nas? 0\n",
      "[] []\n",
      "Any nas? 0\n"
     ]
    }
   ],
   "source": [
    "#ckpt='1600'\n",
    "how_to_fill_na = 1\n",
    "if real_predict_test == 1:\n",
    "    pred_df = pd.read_csv(f'./outputs/corrected_models/HALF_predict_test_muppet-roberta-base-finetuned-squad-{args[\"split\"]}-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}-warm_{args[\"warmup_ratio\"]}-seed_{args[\"seed\"]}-{args[\"special\"]}-checkpoint-{ckpt}.csv', names=[\"id\", \"q'\", \"r'\"], dtype=str)\n",
    "    valid_data = pd.read_csv(\"./dataset/test.csv\")\n",
    "    print('Any nas?', pred_df.isna().sum().sum())\n",
    "    \n",
    "    if how_to_fill_na == 0:\n",
    "        pred_df.fillna(value='', inplace=True)\n",
    "    elif how_to_fill_na == 1:\n",
    "        r, _ = np.where(pred_df.isna())\n",
    "        print(r, _)\n",
    "        for i in range(len(r)):\n",
    "            for j in range(valid_data.shape[0]):\n",
    "                if valid_data.iloc[j]['id'] == int(pred_df.iloc[r[i]][0]):\n",
    "                    getq = valid_data.iloc[j][\"q\"]\n",
    "                    getr = valid_data.iloc[j][\"r\"]\n",
    "                    if _[i] == 1:\n",
    "                        pred_df.iloc[r[i]][\"q'\"] = getq\n",
    "                    elif _[i] == 2:\n",
    "                        pred_df.iloc[r[i]][\"r'\"] = getr\n",
    "                    break\n",
    "    elif how_to_fill_na == 2:\n",
    "        helper_df = pd.read_csv('./outputs/others/11-ckwu-roberta_large_cvseed-3_idx-2_lr-3e-05_warmup-6pct_ckpt-3000.csv', names=[\"id\", \"q\", \"r\"], dtype=str)\n",
    "        r, _ = np.where(pred_df.isna())\n",
    "        print(helper_df.shape[0], helper_df.iloc[0]['id'])\n",
    "        print(r, _)\n",
    "        for i in range(len(r)):\n",
    "            for j in range(helper_df.shape[0]):\n",
    "                if int(helper_df.iloc[j]['id']) == int(pred_df.iloc[r[i]][0]):\n",
    "                    getq = helper_df.iloc[j][\"q\"].replace('\"','')\n",
    "                    getr = helper_df.iloc[j][\"r\"].replace('\"','')\n",
    "                    if _[i] == 1 and getq != '':\n",
    "                        pred_df.iloc[r[i]][\"q'\"] = getq\n",
    "                    elif _[i] == 2 and getr != '':\n",
    "                        pred_df.iloc[r[i]][\"r'\"] = getr\n",
    "                    break\n",
    "    # fill na in df\n",
    "    print('Any nas?', pred_df.isna().sum().sum())\n",
    "\n",
    "    pred_df = pred_df.rename({\"q'\": 'q', \"r'\": 'r'}, axis=1)\n",
    "    pred_df.loc[:, ['q', 'r']] = pred_df[['q', 'r']].applymap(lambda s: '\"' + str(s).strip('\"') + '\"')\n",
    "    pred_df.to_csv(f'./outputs/corrected_models/REAL_predict_test_muppet-roberta-base-finetuned-squad-{args[\"split\"]}-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}-warm_{args[\"warmup_ratio\"]}-seed_{args[\"seed\"]}-checkpoint-{ckpt}.csv', header=True, quotechar='\"', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a4f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "61106ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_gold_csv(valid_data, 'split6+22.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff97f0",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c861eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict\n",
    "from typing import List, Tuple, Any, Union\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from transformers import EvalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1cba764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenize(text: str, filter_puncts: bool = True) -> List[str]:\n",
    "    punctuations = set([ch for ch in \"!\\\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\"])\n",
    "    text = text.strip('\"') # NOTE: remove the quotes first\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    if filter_puncts:\n",
    "        tokens = list(filter(lambda t: t not in punctuations, tokens))\n",
    "    return tokens\n",
    "    \n",
    "def longestCommonSubsequence(text1: list, text2: list) -> int:\n",
    "    if len(text2) > len(text1):\n",
    "        text1, text2 = text2, text1\n",
    "\n",
    "    lcs = [[0] * (len(text2) + 1) for _ in range(2)]\n",
    "    for i in range(1, len(text1)+1):\n",
    "        for j in range(1, len(text2)+1):\n",
    "            if text1[i-1] == text2[j-1]:\n",
    "                lcs[i % 2][j] = lcs[(i-1) % 2][j-1] + 1\n",
    "            else:\n",
    "                lcs[i % 2][j] = max(lcs[(i-1) % 2][j], lcs[i % 2][j-1])\n",
    "\n",
    "    return lcs[len(text1) % 2][len(text2)]\n",
    "\n",
    "def compute_lcs_score(pred: list, ans: list) -> float:\n",
    "    intersection = longestCommonSubsequence(pred, ans)\n",
    "    union = len(pred) + len(ans) - intersection\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    lcs_score = intersection / union\n",
    "    if (lcs_score < 0) or (lcs_score) > 1:\n",
    "        raise ValueError(\"LCS score must be between 0 and 1\")\n",
    "    return lcs_score\n",
    "\n",
    "def compute_lcs_scores(pred_df: pd.DataFrame, ans_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ids, qp_scores, rp_scores = list(), list(), list()\n",
    "    for _, prow in pred_df.iterrows():\n",
    "        pid, qp_pred, rp_pred = prow[\"id\"], prow[\"q'\"], prow[\"r'\"]\n",
    "        qp_pred, rp_pred = [nltk_tokenize(pred) for pred in [qp_pred, rp_pred]]\n",
    "        ans_rows = ans_df[ans_df.id == pid]\n",
    "\n",
    "        for _, arow in ans_rows.iterrows():\n",
    "            qp_ans, rp_ans = arow[\"q'\"], arow[\"r'\"]\n",
    "            qp_ans, rp_ans = [nltk_tokenize(ans) for ans in [qp_ans, rp_ans]]\n",
    "            qp_score, rp_score = compute_lcs_score(qp_pred, qp_ans), compute_lcs_score(rp_pred, rp_ans)\n",
    "\n",
    "            for item, l in zip([pid, qp_score, rp_score], [ids, qp_scores, rp_scores]):\n",
    "                l.append(item)\n",
    "\n",
    "    assert ids == ans_df.id.tolist()\n",
    "    lcs_df = pd.DataFrame(data={\n",
    "        \"id\": ids,\n",
    "        \"qp_scores\": qp_scores,\n",
    "        \"rp_scores\": rp_scores\n",
    "    })\n",
    "    return lcs_df\n",
    "\n",
    "def compute_final_score(lcs_df: pd.DataFrame) -> float:\n",
    "    lcs_df[\"total_scores\"] = lcs_df[\"qp_scores\"] + lcs_df[\"rp_scores\"]\n",
    "    max_scores = lcs_df.groupby(\"id\")[\"total_scores\"].max()\n",
    "    final_score = max_scores.sum() / (2 * len(max_scores))\n",
    "    if (final_score < 0) or (final_score > 1):\n",
    "        raise ValueError(\"The final score must be between 0 and 1, please check the implementation.\")\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "042f80b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-400 final score: 0.7898045707366026\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-600 final score: 0.7930742405174477\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-800 final score: 0.8049952559754687\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1000 final score: 0.8040372669881446\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1200 final score: 0.8045085022138415\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1400 final score: 0.8043813878888371\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1600 final score: 0.806858283762203\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1800 final score: 0.8017252510026983\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2000 final score: 0.8071309043125297\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2200 final score: 0.8044431428409163\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2400 final score: 0.8021175356177814\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2600 final score: 0.8044867752201994\n",
      "#=======fill na with whole=====\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-400 final score: 0.7898045707366026\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-600 final score: 0.7930742405174477\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-800 final score: 0.8049952559754687\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1000 final score: 0.8040372669881446\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1200 final score: 0.8045085022138415\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1400 final score: 0.8043813878888371\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1600 final score: 0.806858283762203\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1800 final score: 0.8017252510026983\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2000 final score: 0.8071309043125297\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2200 final score: 0.8044431428409163\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2400 final score: 0.8021175356177814\n",
      "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2600 final score: 0.8044867752201994\n",
      "#=======fill na with whole=====\n"
     ]
    }
   ],
   "source": [
    "how_to_fill_nas = [0, 1]\n",
    "# ckpts = ['500', '1000', '1500', '2000', '2500', '3000', '3500']\n",
    "#ckpts = list(range(400, 2800, 200))\n",
    "for how_to_fill_na in how_to_fill_nas:\n",
    "    for ckpt in ckpts:\n",
    "        pred_df = pd.read_csv(f'./outputs/corrected_models/predict_test_muppet-roberta-base-finetuned-squad-{args[\"split\"]}-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}-warm_{args[\"warmup_ratio\"]}-seed_{args[\"seed\"]}-{args[\"special\"]}-checkpoint-{ckpt}.csv', names=[\"id\", \"q'\", \"r'\"], dtype=str)\n",
    "        ans_df = pd.read_csv(\"split6+22.csv\", names=[\"id\", \"q'\", \"r'\"], dtype=str)\n",
    "    #     pred_df = pd.read_csv(f'./outputs/predict_test.csv', names=[\"id\", \"q'\", \"r'\"], dtype=str)\n",
    "    #     ans_df = pd.read_csv(\"test_gold_622.csv\", names=[\"id\", \"q'\", \"r'\"], dtype=str)\n",
    "\n",
    "        #print('Any nas?', pred_df.isna().sum().sum())\n",
    "        if how_to_fill_na == 0:\n",
    "            pred_df.fillna(value='', inplace=True)\n",
    "        elif how_to_fill_na == 1:\n",
    "            r, _ = np.where(pred_df.isna())\n",
    "            for i in range(len(r)):\n",
    "                for j in range(valid_data.shape[0]):\n",
    "                    if valid_data.iloc[j]['id'] == int(pred_df.iloc[r[i]][0]):\n",
    "                        getq = valid_data.iloc[j][\"q\"]\n",
    "                        getr = valid_data.iloc[j][\"r\"]\n",
    "                        if _[i] == 1:\n",
    "                            pred_df.iloc[r[i]][1] = getq\n",
    "                        elif _[i] == 2:\n",
    "                            pred_df.iloc[r[i]][2] = getr\n",
    "                        break\n",
    "        #print('Any nas?', pred_df.isna().sum().sum())\n",
    "\n",
    "        if len(pred_df) != len(ans_df.groupby(\"id\").size()):\n",
    "            raise ValueError(\"The prediction file must have the same number of rows as the number of unique IDs in the answer file\")\n",
    "\n",
    "        lcs_df = compute_lcs_scores(pred_df, ans_df) # has len(ans_df) rows of lcs_q' and lcs_r'\n",
    "        final_score = compute_final_score(lcs_df) # derive the final score by \"1/2N (\\sum_i^N(max_j(score_q' + score_r')))\"\n",
    "        print(f'# {args[\"model_name\"]}-{args[\"split\"]}-ckpt-{ckpt} final score: {final_score}')\n",
    "    print('#=======fill na with whole=====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8363f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### bert-large-uncased 622 #####\n",
    "#500: 0.5845899012911759\n",
    "#1000: 0.5673322547362882\n",
    "#1500: 0.618273462886458\n",
    "#2000: 0.5929113627103975\n",
    "#2500: 0.5836752600091908\n",
    "#3000: 0.5987110975060392\n",
    "#3500: 0.5981470868846762\n",
    "#4000: 0.5981470868846762\n",
    "#4500: 0.5907715976593922\n",
    "\n",
    "#### bert-base 82 ####\n",
    "# bert-base-uncased-checkpoint-3000 final test score: 0.73248\n",
    "# bert-base-uncased-checkpoint-3500 final test score: 0.738717087187761\n",
    "# bert-base-uncased-checkpoint-4000 final test score: 0.7291726756958699\n",
    "# bert-base-uncased-checkpoint-4500 final test score: 0.7372288863123244\n",
    "# bert-base-uncased-checkpoint-5000 final test score: 0.7349707304725923\n",
    "# bert-base-uncased-checkpoint-5500 final test score: 0.7308097458575068\n",
    "# bert-base-uncased-checkpoint-6000 final test score: 0.7300140861374874\n",
    "\n",
    "### roberta-large 82 b_4 lr_1.5e-5 ###\n",
    "# roberta-large-checkpoint-500 final test score: 0.7257905628659318\n",
    "# roberta-large-checkpoint-1000 final test score: 0.7568639627720252\n",
    "# roberta-large-checkpoint-1500 final test score: 0.725855670985741\n",
    "# roberta-large-checkpoint-2000 final test score: 0.7357508737312826\n",
    "# roberta-large-checkpoint-2500 final test score: 0.7247474600774396\n",
    "# roberta-large-checkpoint-3000 final test score: 0.7230775245966534\n",
    "# roberta-large-checkpoint-3500 final test score: 0.6832446910327563\n",
    "\n",
    "### roberta-large 82 b_4 lr_3e-5\n",
    "# roberta-large-checkpoint-500 final test score: 0.7090992655072061\n",
    "# roberta-large-checkpoint-1000 final test score: 0.7353674714986471\n",
    "# roberta-large-checkpoint-1500 final test score: 0.7252969458937408\n",
    "# roberta-large-checkpoint-2000 final test score: 0.7479845181818972\n",
    "# roberta-large-checkpoint-2500 final test score: 0.7540305270800959\n",
    "# roberta-large-checkpoint-3000 final test score: 0.7466347808457232\n",
    "# roberta-large-checkpoint-3500 final test score: 0.7517260866741773\n",
    "# --- with nan fill\n",
    "# roberta-large-checkpoint-500 final test score: 0.7247626604212086\n",
    "# roberta-large-checkpoint-1000 final test score: 0.7368753560158637\n",
    "# roberta-large-checkpoint-1500 final test score: 0.7308504918469712\n",
    "# roberta-large-checkpoint-2000 final test score: 0.7501619558881207\n",
    "# roberta-large-checkpoint-2500 final test score: 0.7560358746736786\n",
    "# roberta-large-checkpoint-3000 final test score: 0.7493604083679714\n",
    "# roberta-large-checkpoint-3500 final test score: 0.7546514831383723\n",
    "\n",
    "### roberta-large 82 b_4 lr_1.5e-5 warm_0.06 seed 42\n",
    "# roberta-large-checkpoint-600 final test score: 0.7254769792233624\n",
    "# roberta-large-checkpoint-800 final test score: 0.7509314441409943\n",
    "# roberta-large-checkpoint-1000 final test score: 0.7375322469505107\n",
    "# roberta-large-checkpoint-1200 final test score: 0.7422551023687668\n",
    "# roberta-large-checkpoint-1400 final test score: 0.7531907527812947\n",
    "# roberta-large-checkpoint-1600 final test score: 0.7368397793987354\n",
    "# roberta-large-checkpoint-1800 final test score: 0.746870872686432\n",
    "# roberta-large-checkpoint-2000 final test score: 0.753262026941743\n",
    "# roberta-large-checkpoint-2200 final test score: 0.762348043931238\n",
    "# roberta-large-checkpoint-2400 final test score: 0.7523878335254599\n",
    "# roberta-large-checkpoint-2600 final test score: 0.7545938197649968\n",
    "# roberta-large-checkpoint-2800 final test score: 0.75427218198298\n",
    "# roberta-large-checkpoint-3000 final test score: 0.7516087253918616\n",
    "# roberta-large-checkpoint-3200 final test score: 0.7540679019577344\n",
    "# roberta-large-checkpoint-3400 final test score: 0.7446163572437777\n",
    "# roberta-large-checkpoint-3600 final test score: 0.7566421271082923\n",
    "# --- with nan fill\n",
    "# roberta-large-checkpoint-600 final test score: 0.7313529963712819\n",
    "# roberta-large-checkpoint-800 final test score: 0.7545260799321256\n",
    "# roberta-large-checkpoint-1000 final test score: 0.7380537321403313\n",
    "# roberta-large-checkpoint-1200 final test score: 0.7453525889456024\n",
    "# roberta-large-checkpoint-1400 final test score: 0.7588837647477735\n",
    "# roberta-large-checkpoint-1600 final test score: 0.7379924987069616\n",
    "# roberta-large-checkpoint-1800 final test score: 0.7499828003916013\n",
    "# roberta-large-checkpoint-2000 final test score: 0.7542079208475869\n",
    "# roberta-large-checkpoint-2200 final test score: 0.7635827604037518\n",
    "# roberta-large-checkpoint-2400 final test score: 0.7547314397176267\n",
    "# roberta-large-checkpoint-2600 final test score: 0.7569374259571638\n",
    "# roberta-large-checkpoint-2800 final test score: 0.7617093213975912\n",
    "# roberta-large-checkpoint-3000 final test score: 0.755765971288782\n",
    "# roberta-large-checkpoint-3200 final test score: 0.7611585480659026\n",
    "# roberta-large-checkpoint-3400 final test score: 0.7495593170443721\n",
    "# roberta-large-checkpoint-3600 final test score: 0.7624908243438382\n",
    "\n",
    "\n",
    "### roberta-large 622valid b_4 lr_3e-5 warm_0.06 seed 42\n",
    "### 0 post\n",
    "# roberta-large-checkpoint-600 final test score: 0.6500376887617668\n",
    "# roberta-large-checkpoint-800 final test score: 0.6683186190128978\n",
    "# roberta-large-checkpoint-1000 final test score: 0.7084029250350091\n",
    "# roberta-large-checkpoint-1200 final test score: 0.7140576600198701\n",
    "# roberta-large-checkpoint-1400 final test score: 0.7025639686006379\n",
    "# roberta-large-checkpoint-1600 final test score: 0.7306867263515114\n",
    "# roberta-large-checkpoint-1800 final test score: 0.6997363305052697\n",
    "# roberta-large-checkpoint-2000 final test score: 0.713913067718691\n",
    "# roberta-large-checkpoint-2200 final test score: 0.7351097835486486\n",
    "# roberta-large-checkpoint-2400 final test score: 0.7564515992217088\n",
    "# roberta-large-checkpoint-2600 final test score: 0.7623857063025837\n",
    "# roberta-large-checkpoint-2800 final test score: 0.7641920976533865\n",
    "# roberta-large-checkpoint-3000 final test score: 0.7704696176038958\n",
    "# roberta-large-checkpoint-3200 final test score: 0.7689731713834116\n",
    "# roberta-large-checkpoint-3400 final test score: 0.763272945108771\n",
    "# roberta-large-checkpoint-3600 final test score: 0.777026525952762\n",
    "# roberta-large-checkpoint-3800 final test score: 0.7777010637854648\n",
    "######### remove char after period\n",
    "# roberta-large-checkpoint-600 final test score: 0.6740745295817693\n",
    "# roberta-large-checkpoint-800 final test score: 0.6912120244156451\n",
    "# roberta-large-checkpoint-1000 final test score: 0.7326908515772249\n",
    "# roberta-large-checkpoint-1200 final test score: 0.7385665228015272\n",
    "# roberta-large-checkpoint-1400 final test score: 0.7262132584232915\n",
    "# roberta-large-checkpoint-1600 final test score: 0.7558633975654591\n",
    "# roberta-large-checkpoint-1800 final test score: 0.725013460127671\n",
    "# roberta-large-checkpoint-2000 final test score: 0.7392653076153044\n",
    "# roberta-large-checkpoint-2200 final test score: 0.7600332405505135\n",
    "# roberta-large-checkpoint-2400 final test score: 0.7810522283949544\n",
    "# roberta-large-checkpoint-2600 final test score: 0.7881007482584063\n",
    "# roberta-large-checkpoint-2800 final test score: 0.7906632047340922\n",
    "# roberta-large-checkpoint-3000 final test score: 0.7944929732379744\n",
    "# roberta-large-checkpoint-3200 final test score: 0.7935074650035119\n",
    "# roberta-large-checkpoint-3400 final test score: 0.788747348261974\n",
    "# roberta-large-checkpoint-3600 final test score: 0.799180242654962\n",
    "# roberta-large-checkpoint-3800 final test score: 0.8002754357899929\n",
    "#roberta-large-checkpoint-4000 final test score: 0.795986979623897\n",
    "# roberta-large-checkpoint-4200 final test score: 0.7961234418364423\n",
    "\n",
    "### roberta-large 622test b_4 lr_3e-5 warm_0.06 seed 42\n",
    "# roberta-large-checkpoint-2000 final test score: 0.7279140946133679\n",
    "# roberta-large-checkpoint-2200 final test score: 0.7429188422388359\n",
    "# roberta-large-checkpoint-2400 final test score: 0.7642529418099401\n",
    "# roberta-large-checkpoint-2600 final test score: 0.7654203286241084\n",
    "# roberta-large-checkpoint-2800 final test score: 0.7679979745272225\n",
    "# roberta-large-checkpoint-3000 final test score: 0.7808798827981996\n",
    "# roberta-large-checkpoint-3200 final test score: 0.7774418167785709\n",
    "# roberta-large-checkpoint-3400 final test score: 0.771937339919355\n",
    "# roberta-large-checkpoint-3600 final test score: 0.7832835233229676\n",
    "# roberta-large-checkpoint-3800 final test score: 0.7825181480287842\n",
    "\n",
    "### roberta-large 622 valid b_4 lr_3e-5 warm_0.06 shuffle corrected seed 42 \n",
    "#==== raw\n",
    "# roberta-large-checkpoint-200 final test score: 0.6249067319603748\n",
    "# roberta-large-checkpoint-400 final test score: 0.7508643564681374\n",
    "# roberta-large-checkpoint-600 final test score: 0.7638696318401204\n",
    "# roberta-large-checkpoint-800 final test score: 0.7541996657066029\n",
    "# roberta-large-checkpoint-1000 final test score: 0.767665948217005\n",
    "# roberta-large-checkpoint-1200 final test score: 0.7666249742193298\n",
    "# roberta-large-checkpoint-1400 final test score: 0.7584889936916765\n",
    "# roberta-large-checkpoint-1600 final test score: 0.7832800173610261\n",
    "# roberta-large-checkpoint-1800 final test score: 0.7563130996739712\n",
    "# roberta-large-checkpoint-2000 final test score: 0.7766218727076956\n",
    "# roberta-large-checkpoint-2200 final test score: 0.7615458247207753\n",
    "# roberta-large-checkpoint-2400 final test score: 0.7611164912747018\n",
    "# roberta-large-checkpoint-2600 final test score: 0.7495980436303449\n",
    "# roberta-large-checkpoint-2800 final test score: 0.7531305793656844\n",
    "# roberta-large-checkpoint-3000 final test score: 0.7503282049931337\n",
    "# roberta-large-checkpoint-3200 final test score: 0.7497488083470066\n",
    "# roberta-large-checkpoint-3400 final test score: 0.7468628734762588\n",
    "# roberta-large-checkpoint-3600 final test score: 0.73780700308036\n",
    "# roberta-large-checkpoint-3800 final test score: 0.7378683993988545\n",
    "#==== fill na with orig <p.s. remove period is useless>\n",
    "# roberta-large-checkpoint-200 final test score: 0.6312645786034125\n",
    "# roberta-large-checkpoint-400 final test score: 0.7565967394376918\n",
    "# roberta-large-checkpoint-600 final test score: 0.779170971359442\n",
    "# roberta-large-checkpoint-800 final test score: 0.7652257230489458\n",
    "# roberta-large-checkpoint-1000 final test score: 0.7778082605503085\n",
    "# roberta-large-checkpoint-1200 final test score: 0.7701981090570195\n",
    "# roberta-large-checkpoint-1400 final test score: 0.7667329485490181\n",
    "# roberta-large-checkpoint-1600 final test score: 0.7864791857407051\n",
    "# roberta-large-checkpoint-1800 final test score: 0.7578354512534279\n",
    "# roberta-large-checkpoint-2000 final test score: 0.7812647161563059\n",
    "# roberta-large-checkpoint-2200 final test score: 0.7660047408029417\n",
    "# roberta-large-checkpoint-2400 final test score: 0.7629594586149209\n",
    "# roberta-large-checkpoint-2600 final test score: 0.7522053325432061\n",
    "# roberta-large-checkpoint-2800 final test score: 0.7615965720846072\n",
    "# roberta-large-checkpoint-3000 final test score: 0.7603165456396559\n",
    "# roberta-large-checkpoint-3200 final test score: 0.7586114583375456\n",
    "# roberta-large-checkpoint-3400 final test score: 0.755211742035463\n",
    "# roberta-large-checkpoint-3600 final test score: 0.7474518808556296\n",
    "# roberta-large-checkpoint-3800 final test score: 0.7475132771741244\n",
    "\n",
    "### roberta-large 622 test b_4 lr_3e-5 warm_0.06 shuffle corrected seed 42 \n",
    "# raw\n",
    "# roberta-large-checkpoint-400 final test score: 0.757497569412286\n",
    "# roberta-large-checkpoint-600 final test score: 0.7664141200048084\n",
    "# roberta-large-checkpoint-800 final test score: 0.7643846334898196\n",
    "# roberta-large-checkpoint-1000 final test score: 0.7714245732422911\n",
    "# roberta-large-checkpoint-1200 final test score: 0.7678900789020857\n",
    "# roberta-large-checkpoint-1400 final test score: 0.7649841945293655\n",
    "# roberta-large-checkpoint-1600 final test score: 0.7880313027732246\n",
    "# roberta-large-checkpoint-1800 final test score: 0.7681806911747383\n",
    "# roberta-large-checkpoint-2000 final test score: 0.7848556635092381\n",
    "# roberta-large-checkpoint-2200 final test score: 0.7622602800421855\n",
    "# roberta-large-checkpoint-2400 final test score: 0.7678715310868255\n",
    "# roberta-large-checkpoint-2600 final test score: 0.7540259758095897\n",
    "# roberta-large-checkpoint-2800 final test score: 0.7610216214665433\n",
    "# roberta-large-checkpoint-3000 final test score: 0.756047131549043\n",
    "# fill na with orig <p.s. remove period is useless>\n",
    "# roberta-large-checkpoint-400 final test score: 0.7649961672626635\n",
    "# roberta-large-checkpoint-600 final test score: 0.7838210361872578\n",
    "# roberta-large-checkpoint-800 final test score: 0.7767477535396258\n",
    "# roberta-large-checkpoint-1000 final test score: 0.7840407000138133\n",
    "# roberta-large-checkpoint-1200 final test score: 0.771052574827685\n",
    "# roberta-large-checkpoint-1400 final test score: 0.7721745679186325\n",
    "# roberta-large-checkpoint-1600 final test score: 0.7939244426455291\n",
    "# roberta-large-checkpoint-1800 final test score: 0.7698764619062523\n",
    "# roberta-large-checkpoint-2000 final test score: 0.7883790070826443\n",
    "# roberta-large-checkpoint-2200 final test score: 0.7685556064060656\n",
    "# roberta-large-checkpoint-2400 final test score: 0.770218012184319\n",
    "# roberta-large-checkpoint-2600 final test score: 0.7597592791156415\n",
    "# roberta-large-checkpoint-2800 final test score: 0.7700800081365856\n",
    "# roberta-large-checkpoint-3000 final test score: 0.7673177023569069\n",
    "\n",
    "# roberta-large 82_3_1 test b_8 lr_3e-5 warm_0.06 shuffle corrected seed 42 \n",
    "# raw\n",
    "# roberta-large-82_3_1-ckpt-400 final score: 0.7654934567233056\n",
    "# roberta-large-82_3_1-ckpt-600 final score: 0.7491009976348183\n",
    "# roberta-large-82_3_1-ckpt-800 final score: 0.7433897649619771\n",
    "# roberta-large-82_3_1-ckpt-1000 final score: 0.7449358625638374\n",
    "# roberta-large-82_3_1-ckpt-1200 final score: 0.7750203592650212\n",
    "# roberta-large-82_3_1-ckpt-1400 final score: 0.7569425023616523\n",
    "# roberta-large-82_3_1-ckpt-1600 final score: 0.7757300583214787\n",
    "# roberta-large-82_3_1-ckpt-1800 final score: 0.7679848981011869\n",
    "# roberta-large-82_3_1-ckpt-2000 final score: 0.7630122421414384\n",
    "# roberta-large-82_3_1-ckpt-2200 final score: 0.7688467340317298\n",
    "# roberta-large-82_3_1-ckpt-2400 final score: 0.769385044338644\n",
    "# roberta-large-82_3_1-ckpt-2600 final score: 0.7591458679829894\n",
    "# roberta-large-82_3_1-ckpt-2800 final score: 0.7639454750323195\n",
    "#=======fill na with whole=====\n",
    "# roberta-large-82_3_1-ckpt-400 final score: 0.7700512372823377\n",
    "# roberta-large-82_3_1-ckpt-600 final score: 0.7523299842649315\n",
    "# roberta-large-82_3_1-ckpt-800 final score: 0.7610256312565378\n",
    "# roberta-large-82_3_1-ckpt-1000 final score: 0.7833575452485532\n",
    "# roberta-large-82_3_1-ckpt-1200 final score: 0.7787087199794752\n",
    "# roberta-large-82_3_1-ckpt-1400 final score: 0.7687006257178984\n",
    "# roberta-large-82_3_1-ckpt-1600 final score: 0.7799043597215102\n",
    "# roberta-large-82_3_1-ckpt-1800 final score: 0.7770942483714922\n",
    "# roberta-large-82_3_1-ckpt-2000 final score: 0.782135095103325\n",
    "# roberta-large-82_3_1-ckpt-2200 final score: 0.7700982984872993\n",
    "# roberta-large-82_3_1-ckpt-2400 final score: 0.7719846231772717\n",
    "# roberta-large-82_3_1-ckpt-2600 final score: 0.7673873642703445\n",
    "# roberta-large-82_3_1-ckpt-2800 final score: 0.7798807741778196\n",
    "\n",
    "# roberta-large 82_3_0 test b_8 lr_3e-5 warm_0.06 shuffle corrected seed 42 \n",
    "# raw\n",
    "# roberta-large-82_3_0-ckpt-400 final score: 0.7519655552098666\n",
    "# roberta-large-82_3_0-ckpt-600 final score: 0.7607480517889734\n",
    "# roberta-large-82_3_0-ckpt-800 final score: 0.7686799960586809\n",
    "# roberta-large-82_3_0-ckpt-1000 final score: 0.7552695630703136\n",
    "# roberta-large-82_3_0-ckpt-1200 final score: 0.779041918260717\n",
    "# roberta-large-82_3_0-ckpt-1400 final score: 0.7610133622889663\n",
    "# roberta-large-82_3_0-ckpt-1600 final score: 0.7648633883452065\n",
    "# roberta-large-82_3_0-ckpt-1800 final score: 0.7493947443757345\n",
    "# roberta-large-82_3_0-ckpt-2000 final score: 0.7620568353519128\n",
    "# roberta-large-82_3_0-ckpt-2200 final score: 0.756504689717084\n",
    "# roberta-large-82_3_0-ckpt-2400 final score: 0.7686461644857229\n",
    "# roberta-large-82_3_0-ckpt-2600 final score: 0.7662339209225864\n",
    "# roberta-large-82_3_0-ckpt-2800 final score: 0.7625701341092707\n",
    "# =======fill na with whole=====\n",
    "# roberta-large-82_3_0-ckpt-400 final score: 0.7572833652679771\n",
    "# roberta-large-82_3_0-ckpt-600 final score: 0.7686975376544027\n",
    "# roberta-large-82_3_0-ckpt-800 final score: 0.7812658578460477\n",
    "# roberta-large-82_3_0-ckpt-1000 final score: 0.7710109601822083\n",
    "# roberta-large-82_3_0-ckpt-1200 final score: 0.7793548093746094\n",
    "# roberta-large-82_3_0-ckpt-1400 final score: 0.7639358064626044\n",
    "# roberta-large-82_3_0-ckpt-1600 final score: 0.7735388317794816\n",
    "# roberta-large-82_3_0-ckpt-1800 final score: 0.7816834187917098\n",
    "# roberta-large-82_3_0-ckpt-2000 final score: 0.7789240670929582\n",
    "# roberta-large-82_3_0-ckpt-2200 final score: 0.7807226655952566\n",
    "# roberta-large-82_3_0-ckpt-2400 final score: 0.7698081404599644\n",
    "# roberta-large-82_3_0-ckpt-2600 final score: 0.7681112676059405\n",
    "# roberta-large-82_3_0-ckpt-2800 final score: 0.7653521886860211\n",
    "\n",
    "#roberta-large 6+22 test b_8 lr_3e-5 warm_0.06 shuffle corrected seed 42 \n",
    "# roberta-large-6+22-ckpt-1000 final score: 0.7938537868578737\n",
    "# roberta-large-6+22-ckpt-1200 final score: 0.7937386959154632\n",
    "# roberta-large-6+22-ckpt-1400 final score: 0.7945439923747236\n",
    "# roberta-large-6+22-ckpt-1600 final score: 0.7893022446707817\n",
    "# roberta-large-6+22-ckpt-1800 final score: 0.7840754552778941\n",
    "#=======fill na with whole=====\n",
    "# roberta-large-6+22-ckpt-1000 final score: 0.7938537868578737\n",
    "# roberta-large-6+22-ckpt-1200 final score: 0.7937386959154632\n",
    "# roberta-large-6+22-ckpt-1400 final score: 0.7945439923747236\n",
    "# roberta-large-6+22-ckpt-1600 final score: 0.7896151357846741\n",
    "# roberta-large-6+22-ckpt-1800 final score: 0.7840754552778941\n",
    "\n",
    "#roberta-large 6+22 2sep shuffle corrected seed 42 \n",
    "# roberta-large-6+22-ckpt-400 final score: 0.7241265238737301\n",
    "# roberta-large-6+22-ckpt-600 final score: 0.7855164767583794\n",
    "# roberta-large-6+22-ckpt-800 final score: 0.7707064536359541\n",
    "# roberta-large-6+22-ckpt-1000 final score: 0.7570536677382713\n",
    "# roberta-large-6+22-ckpt-1200 final score: 0.788497813842746\n",
    "# roberta-large-6+22-ckpt-1400 final score: 0.7788683772355874\n",
    "# roberta-large-6+22-ckpt-1600 final score: 0.7779577865589014\n",
    "# roberta-large-6+22-ckpt-1800 final score: 0.7869460633150134\n",
    "# roberta-large-6+22-ckpt-2000 final score: 0.7644081397901518\n",
    "#=======fill na with whole=====\n",
    "# roberta-large-6+22-ckpt-400 final score: 0.7241265238737301\n",
    "# roberta-large-6+22-ckpt-600 final score: 0.7855164767583794\n",
    "# roberta-large-6+22-ckpt-800 final score: 0.7715657659111699\n",
    "# roberta-large-6+22-ckpt-1000 final score: 0.7570536677382713\n",
    "# roberta-large-6+22-ckpt-1200 final score: 0.7888107049566384\n",
    "# roberta-large-6+22-ckpt-1400 final score: 0.7788683772355874\n",
    "# roberta-large-6+22-ckpt-1600 final score: 0.7779577865589014\n",
    "# roberta-large-6+22-ckpt-1800 final score: 0.7869460633150134\n",
    "# roberta-large-6+22-ckpt-2000 final score: 0.7644081397901518\n",
    "\n",
    "#roberta-large 622 2input shuffle corrected seed 42 valid\n",
    "# roberta-large-622-ckpt-400 final score: 0.5798392303645943\n",
    "# roberta-large-622-ckpt-600 final score: 0.5321390578059124\n",
    "# roberta-large-622-ckpt-800 final score: 0.6845253001491609\n",
    "# roberta-large-622-ckpt-1000 final score: 0.7854346665974302\n",
    "# roberta-large-622-ckpt-1200 final score: 0.7354850139916187\n",
    "# roberta-large-622-ckpt-1400 final score: 0.7433643910913482\n",
    "# roberta-large-622-ckpt-1600 final score: 0.7644419415086158\n",
    "# roberta-large-622-ckpt-1800 final score: 0.766091247946565\n",
    "# roberta-large-622-ckpt-2000 final score: 0.7608942064589528\n",
    "# roberta-large-622-ckpt-2200 final score: 0.7669459531461037\n",
    "# roberta-large-622-ckpt-2400 final score: 0.7684356210623894\n",
    "# roberta-large-622-ckpt-2600 final score: 0.7739133165758102\n",
    "#=======fill na with whole=====\n",
    "# roberta-large-622-ckpt-400 final score: 0.7061591841282925\n",
    "# roberta-large-622-ckpt-600 final score: 0.7045455611931878\n",
    "# roberta-large-622-ckpt-800 final score: 0.7121300855904636\n",
    "# roberta-large-622-ckpt-1000 final score: 0.7859745171352401\n",
    "# roberta-large-622-ckpt-1200 final score: 0.7736435346030568\n",
    "# roberta-large-622-ckpt-1400 final score: 0.7691571510371267\n",
    "# roberta-large-622-ckpt-1600 final score: 0.7654667732729589\n",
    "# roberta-large-622-ckpt-1800 final score: 0.766237055205639\n",
    "# roberta-large-622-ckpt-2000 final score: 0.7613063069504207\n",
    "# roberta-large-622-ckpt-2200 final score: 0.7683260083515128\n",
    "# roberta-large-622-ckpt-2400 final score: 0.768654644842114\n",
    "# roberta-large-622-ckpt-2600 final score: 0.774205961794098\n",
    "\n",
    "#roberta-large 622 2input shuffle corrected seed 42 test\n",
    "# roberta-large-622-ckpt-400 final score: 0.7770712127379288\n",
    "# roberta-large-622-ckpt-600 final score: 0.7892216469190373\n",
    "# roberta-large-622-ckpt-800 final score: 0.695481349547146\n",
    "# roberta-large-622-ckpt-1000 final score: 0.7947943177366188\n",
    "# roberta-large-622-ckpt-1200 final score: 0.7489569764741351\n",
    "# roberta-large-622-ckpt-1400 final score: 0.7486675718582444\n",
    "# roberta-large-622-ckpt-1600 final score: 0.7746306127505268\n",
    "# roberta-large-622-ckpt-1800 final score: 0.7713150616045515\n",
    "# roberta-large-622-ckpt-2000 final score: 0.7746301007589347\n",
    "# roberta-large-622-ckpt-2200 final score: 0.7781460743121015\n",
    "# roberta-large-622-ckpt-2400 final score: 0.777065636946207\n",
    "# roberta-large-622-ckpt-2600 final score: 0.7801924167026092\n",
    "#=======fill na with whole=====\n",
    "# roberta-large-622-ckpt-400 final score: 0.7780900644275407\n",
    "# roberta-large-622-ckpt-600 final score: 0.7895241083291333\n",
    "# roberta-large-622-ckpt-800 final score: 0.7218455129517211\n",
    "# roberta-large-622-ckpt-1000 final score: 0.7951135327529325\n",
    "# roberta-large-622-ckpt-1200 final score: 0.7850521502872703\n",
    "# roberta-large-622-ckpt-1400 final score: 0.7743813346589259\n",
    "# roberta-large-622-ckpt-1600 final score: 0.7757153019453537\n",
    "# roberta-large-622-ckpt-1800 final score: 0.7716887585732569\n",
    "# roberta-large-622-ckpt-2000 final score: 0.7751387625720191\n",
    "# roberta-large-622-ckpt-2200 final score: 0.7803026105440398\n",
    "# roberta-large-622-ckpt-2400 final score: 0.7779258223474501\n",
    "# roberta-large-622-ckpt-2600 final score: 0.7808022892127383\n",
    "\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22 shuffle seed42\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-400 final score: 0.791835906091792\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-600 final score: 0.7847847802544615\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-800 final score: 0.7858727372469431\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1000 final score: 0.7947897598828904\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1200 final score: 0.7978136932439455\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1400 final score: 0.79244138941681\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1600 final score: 0.7975915095342249\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1800 final score: 0.7957700059979252\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2000 final score: 0.789255788341949\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2200 final score: 0.8001138267063729\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2400 final score: 0.793329970804117\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2600 final score: 0.7952500272784631\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2800 final score: 0.793452868630505\n",
    "#=======fill na with whole=====\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-400 final score: 0.7924697205302017\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-600 final score: 0.7847847802544615\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-800 final score: 0.7873033732107745\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1000 final score: 0.7960128796917424\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1200 final score: 0.7984110308250127\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1400 final score: 0.7942902914534467\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1600 final score: 0.7979044006481172\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1800 final score: 0.7960828971118176\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2000 final score: 0.7903676579769726\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2200 final score: 0.8010240554013325\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2400 final score: 0.7939273083851843\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2600 final score: 0.7963325633715592\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2800 final score: 0.7943630973254646\n",
    "\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22 shuffle seed24\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-400 final score: 0.7898045707366026\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-600 final score: 0.7930742405174477\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-800 final score: 0.8049952559754687\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1000 final score: 0.8040372669881446\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1200 final score: 0.8045085022138415\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1400 final score: 0.8043813878888371\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1600 final score: 0.806858283762203\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1800 final score: 0.8017252510026983\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2000 final score: 0.8071309043125297\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2200 final score: 0.8044431428409163\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2400 final score: 0.8021175356177814\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2600 final score: 0.8044867752201994\n",
    "#=======fill na with whole=====\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-400 final score: 0.7898045707366026\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-600 final score: 0.7930742405174477\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-800 final score: 0.8049952559754687\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1000 final score: 0.8040372669881446\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1200 final score: 0.8045085022138415\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1400 final score: 0.8043813878888371\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1600 final score: 0.806858283762203\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-1800 final score: 0.8017252510026983\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2000 final score: 0.8071309043125297\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2200 final score: 0.8044431428409163\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2400 final score: 0.8021175356177814\n",
    "# janeel/muppet-roberta-base-finetuned-squad-6+22-ckpt-2600 final score: 0.8044867752201994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # remove any urls\n",
    "#     for i in range(pred_df.shape[0]):\n",
    "#         while pred_df.iloc[i][\"q'\"].find(\"http : //\") != -1: # There is a url\n",
    "#             print(pred_df.iloc[i][\"q'\"])\n",
    "#             if pred_df.iloc[i][\"q'\"].find(\" \", 9 + pred_df.iloc[i][\"q'\"].find(\"http : //\")) != -1: #\n",
    "#                 pred_df.iloc[i][\"q'\"] = pred_df.iloc[i][\"q'\"][:pred_df.iloc[i][\"q'\"].find(\"http : //\")] + pred_df.iloc[i][\"q'\"][pred_df.iloc[i][\"q'\"].find(\" \", 9 + pred_df.iloc[i][\"q'\"].find(\"http : //\"))+1:len(pred_df.iloc[i][\"q'\"])]\n",
    "#             else:\n",
    "#                 pred_df.iloc[i][\"q'\"] = pred_df.iloc[i][\"q'\"][:pred_df.iloc[i][\"q'\"].find(\"http : //\")]\n",
    "#             print(\"New: \",pred_df.iloc[i][\"q'\"])\n",
    "\n",
    "#     print('Any nas?', pred_df.isna().sum().sum())\n",
    "    \n",
    "    # remove the only word after the last period if any\n",
    "#     for i in range(pred_df.shape[0]):\n",
    "#         tempq = str(pred_df.iloc[i][\"q'\"]).rsplit(' ', 1)[0]\n",
    "#         tempr = str(pred_df.iloc[i][\"r'\"]).rsplit(' ', 1)[0]\n",
    "#         pred_df.iloc[i][\"q'\"] = tempq\n",
    "#         pred_df.iloc[i][\"r'\"] = tempr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d133c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
